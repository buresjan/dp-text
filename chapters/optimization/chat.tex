\chapter{Mathematical optimization}\label{optimization}

Mathematical optimization, also known as mathematical programming, is a vast and diverse field addressing problems that arise in engineering, finance, logistics, and many other areas. The methods developed within this field range from classical gradient-based approaches, suitable when analytic derivatives are readily available, to sophisticated derivative-free strategies that handle black-box scenarios where the objective function is expensive or difficult to differentiate.

Choosing an appropriate optimization method for a given problem is a critical step, as problem characteristics such as nonlinearity, the presence of constraints, and the computational cost of evaluating the objective function can significantly influence the method's performance. In some cases, classic gradient-based algorithms—like Davidon-Fletcher-Powell \cite{Fletcher1963} or Broyden-Fletcher-Goldfarb-Shanno \cite{broyden1970}—are efficient and well-understood. However, they are less suitable when gradients must be computed numerically at a high cost, as can happen when the objective function is only available through complex and time-consuming simulations.

In this work, we tackle a constrained, nonlinear optimization problem where the objective function is obtained numerically and may take several hours to evaluate. This setting naturally leads us toward gradient-free (or derivative-free) methods, which rely solely on objective function evaluations. Furthermore, in the presence of constraints, penalty and barrier methods provide systematic ways to handle feasible regions while maintaining the simplicity of an unconstrained search scheme.

However, derivative-free methods must also be chosen carefully. Earlier research efforts \cite{buresBP, buresVU} explored coupling various optimization algorithms—including gradient-based methods—with the numerical simulations underlying our objective function. These studies showed that while gradient-based methods can be effective when gradients are well-defined and efficiently computed, they were less practical in our black-box context. In contrast, gradient-free approaches, which avoid the cost and complexity of gradient approximation, proved to be more robust and straightforward to implement. Among the tested approaches, the Nelder-Mead method outperformed alternative gradient-free algorithms, making it a natural choice for this work.

To further enhance our optimization framework, we also employ a direct search method known as Mesh Adaptive Direct Search (MADS). MADS has shown broad applicability in constrained black-box optimization problems and can be effectively integrated with penalty/barrier techniques. By comparing Nelder-Mead and MADS, we aim to leverage their complementary strengths—Nelder-Mead’s simplicity and MADS’s systematic handling of constraints and adaptive refinement—in a unified framework that addresses the computational challenges of our problem.

This chapter provides the necessary background to understand and motivate the chosen optimization framework. We begin by defining a general optimization problem, then discuss black-box optimization approaches in more detail. Finally, we introduce our proposed framework, describing how Nelder-Mead and MADS fit into this landscape and why we have selected these methods given the constraints and computational complexity of our particular problem setting.

\input{chapters/optimization/general_optimization_problem.tex}
\input{chapters/optimization/black_box.tex}
\input{chapters/optimization/optim_framework.tex}
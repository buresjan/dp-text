\subsection{Heuristic Methods}\label{heuristic}
Heuristic optimization methods often rely on different predefined rules or even trial and error when seeking the solution of an optimization problem. These methods usually do not guarantee optimal solutions, but they are often effective for finding near-optimal results in a reasonable amount of time. Heuristic methods include genetic algorithms, detailed in \cite{BBO-textbook}, along with various other heuristic approaches.

In this section, however, we will focus on a different widely used heuristic method, the Nelder-Mead method, also known as the simplex method\footnote{A simplex in $ \mathbb{R}^n $ is defined as a bounded convex polytope (a generalization of a polyhedron to any dimension) with a non-empty interior and exactly $ n+1 $ vertices \cite{BBO-textbook}.}\footnote{The term "simplex method" more commonly refers to the algorithm used to find the optimal solution in linear programming. This algorithm was developed by George Dantzig \cite{Dantzig1990}.}\cite{Nelder1965}.

The Nelder-Mead method finds a solution to an optimization problem by iteratively constructing simplexes. The process begins by initializing a starting simplex. The objective function is then evaluated at each vertex of this simplex. In each subsequent iteration, the simplex is transformed in order to move closer to the position of the sought stationary point of the objective function. The transformation of the simplex involves manipulating its points using predefined operations -- expansion, reflection, contraction (inner and outer), and shrinking, which are schematically illustrated in Figure~\ref{fig:NM operations}. 

\begin{figure}[H]
	%	\vspace{5mm}
	\centering
	\includegraphics[width=0.96\textwidth]{figures/neldermead.pdf}
	\vspace{2mm}
	\caption{A schematic representation of the operations used to transform simplexes in the Nelder-Mead method. The vertices generated by applying each operation are shown in red. For clarity, the operations are depicted in $ \mathbb{R}^2 $.}
	%	\vspace{2mm}
	\label{fig:NM operations}
\end{figure}

The transformations performed during each iteration are determined by comparing the function values at the vertices of the simplex. The newly formed simplex shares either exactly one vertex or exactly $ n $ vertices with the simplex from the preceding iteration. The algorithm continues to iteratively transform the simplex until a stopping condition (specified by the user) is met \cite{BBO-textbook}. Details of the Nelder-Mead method, including the algorithm's description and the choice of stopping condition, are discussed in \cite{BBO-textbook, derivative-free-review, Nelder1965}.


\begin{algorithm}
	\caption{Nelder-Mead algorithm}\label{neldermead}
	\begin{algorithmic}[1]
		\Require Initial simplex $Y^0 = \{y^0, y^1, \dots, y^n\}$, function $f: \mathbb{R}^n \to \mathbb{R}$, parameters $\delta^e$, $\delta^{oc}$, $\delta^{ic}$, $\gamma$, iteration counter $k \gets 0$
		\Ensure Approximate minimum of $f$
		
		\Procedure{Nelder-Mead}{$Y^0$}
		\State Reorder $Y^k$ so that $f(y^0) \leq f(y^1) \leq \dots \leq f(y^n)$
		\State Set $f^k_{\text{best}} = f(y^0)$
		
		\While{stopping condition not met}
		\Algphase{Reflection}
		\State Compute centroid $x^c = \frac{1}{n} \sum_{i=0}^{n-1} y^i$
		\State Set reflection point $x^r = x^c + \delta^r (x^c - y^n)$
		\State Compute $f^r = f(x^r)$
		\If{$f^k_{\text{best}} \leq f^r < f(y^{n-1})$}
		\State Set $Y^{k+1} = \{y^0, y^1, \dots, y^{n-1}, x^r\}$
		\State Increment $k \gets k+1$ and continue
		\EndIf
		
		\Algphase{Expansion}
		\If{$f^r < f^k_{\text{best}}$}
		\State Set expansion point $x^e = x^c + \delta^e (x^r - x^c)$
		\State Compute $f^e = f(x^e)$
		\If{$f^e < f^r$}
		\State Set $Y^{k+1} = \{y^0, y^1, \dots, y^{n-1}, x^e\}$
		\Else
		\State Set $Y^{k+1} = \{y^0, y^1, \dots, y^{n-1}, x^r\}$
		\EndIf
		\State Increment $k \gets k+1$ and continue
		\EndIf
		
		\Algphase{Contraction}
		\If{$f^r \geq f(y^n)$}
		\State \textbf{Outside Contraction:} Compute $x^{oc} = x^c + \delta^{oc}(x^c - y^n)$
		\State Compute $f^{oc} = f(x^{oc})$
		\If{$f^{oc} < f(y^n)$}
		\State Set $Y^{k+1} = \{y^0, y^1, \dots, y^{n-1}, x^{oc}\}$
		\Else
		\State \textbf{Shrink:} Set $Y^{k+1} = \{y^0, y^0 + \gamma(y^1 - y^0), \dots, y^0 + \gamma(y^n - y^0)\}$
		\EndIf
		\State Increment $k \gets k+1$ and continue
		\Else
		\State \textbf{Inside Contraction:} Compute $x^{ic} = x^c + \delta^{ic}(x^c - y^n)$
		\State Compute $f^{ic} = f(x^{ic})$
		\If{$f^{ic} < f(y^n)$}
		\State Set $Y^{k+1} = \{y^0, y^1, \dots, y^{n-1}, x^{ic}\}$
		\Else
		\State Set $Y^{k+1} = \{y^0, y^0 + \gamma(y^1 - y^0), \dots, y^0 + \gamma(y^n - y^0)\}$
		\EndIf
		\State Increment $k \gets k+1$ and continue
		\EndIf
		\EndWhile
		\EndProcedure
	\end{algorithmic}
\end{algorithm}


The heuristic nature of the Nelder-Mead method stems from the fact that its principle is based on a somewhat random search of the space using predefined rules. Several iterations of space exploration using simplexes, for a specific choice of initial simplex and a specific function, are shown in Figure \ref{fig:NM}. While the convergence of this method has been proven, it is not guaranteed that the method will always converge to a stationary point \cite{BBO-textbook}. It should be noted that the Nelder-Mead method was primarily developed for unconstrained optimization problems, but it can be adapted for constrained optimization problems using the techniques described in section \ref{unconstrained}.


\begin{figure}[H]
	\vspace{-5mm}
	\begin{subfigure}[b]{0.32\textwidth}
		\centering
		%		trim={<left> <lower> <right> <upper>}
		\includegraphics[width=0.96\textwidth, trim={0 0 0 0}, clip]{figures/nelder1.pdf}
	\end{subfigure}
	\begin{subfigure}[b]{0.32\textwidth}
		\centering
		\includegraphics[width=0.96\textwidth, trim={0 0 0 0}]{figures/nelder2.pdf}
	\end{subfigure}
	\vspace{1mm}
	\begin{subfigure}[b]{0.32\textwidth}
		\centering
		\includegraphics[width=0.96\textwidth, trim={0 0 0 0}]{figures/nelder3.pdf}
	\end{subfigure}
	\vspace{1mm}
	\begin{subfigure}[b]{0.32\textwidth}
		\centering
		\includegraphics[width=0.96\textwidth, trim={0 0 0 0}]{figures/nelder4.pdf}
	\end{subfigure}
	\begin{subfigure}[b]{0.32\textwidth}
		\centering
		\includegraphics[width=0.96\textwidth, trim={0 0 0 0}]{figures/nelder5.pdf}
	\end{subfigure}
	\begin{subfigure}[b]{0.32\textwidth}
		\centering
		\includegraphics[width=0.96\textwidth, trim={0 0 0 0}]{figures/nelder6.pdf}
	\end{subfigure}
	\centering
	\begin{subfigure}[b]{0.32\textwidth}
		\centering
		\includegraphics[width=0.96\textwidth, trim={0 0 0 0}]{figures/nelder7.pdf}
	\end{subfigure}
	\begin{subfigure}[b]{0.32\textwidth}
		\centering
		\includegraphics[width=0.96\textwidth, trim={0 0 0 0}]{figures/nelder8.pdf}
	\end{subfigure}
	\begin{subfigure}[b]{0.32\textwidth}
		\centering
		\includegraphics[width=0.96\textwidth, trim={0 0 0 0}]{figures/nelder9.pdf}
	\end{subfigure}
	\begin{center}
		\begin{subfigure}[b]{0.66\textwidth}
			\centering
			\includegraphics[width=0.985\textwidth, trim={0 6mm 0 9mm}]{figures/nelder.png}
		\end{subfigure}
	\end{center}
	
	\caption{Several iterations of the Nelder-Mead method for a specific choice of the initial simplex when minimizing the function $ x^2 - 4x + y^2 - y - xy + 7 $, with the minimum at the point (3,2) marked by a red cross.}
	\label{fig:NM}
\end{figure}


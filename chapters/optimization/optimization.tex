\chapter{Mathematical optimization}\label{optimization}

%Mathematical optimization, also known as mathematical programming, is a broad field that covers various disciplines, including linear and nonlinear optimization, convex programming, integer programming, and more. The goal of this chapter is to summarize the key concepts and techniques relevant to the scope of this work. Specifically, we will focus on methods for solving optimization problems with constraints and those where the objective function is generally unknown and its evaluation is computationally expensive. Classic methods applicable to unconstrained optimization, such as the Davidon-Fletcher-Powell \cite{Fletcher1963} and Broyden-Fletcher-Goldfarb-Shanno \cite{broyden1970} algorithms, while fundamental, fall outside the scope of this work and details on these methods can be found for instance in \cite{Bert}.
%
%%This chapter focuses on methods that address the specific challenges posed by constrained and generally unknown objective functions. First, a general optimization problem is defined, followed by an introduction to basic techniques for solving constrained problems. Next, black-box optimization methods are discussed. Finally, the proposed optimization framework, including its components and their interconnections, is presented in detail.
%
%This chapter focuses on methods that address the specific challenges posed by constrained and generally unknown objective functions. First, a general optimization problem is defined. Next, black-box optimization methods are discussed. Finally, the proposed optimization framework, including its components and their interconnections, is presented in detail.

%\input{chapters/optimization/general_optimization_problem.tex}
%%\input{chapters/optimization/constrained_problems.tex}
%\input{chapters/optimization/black_box.tex}
%\input{chapters/optimization/optim_framework.tex}

Mathematical optimization, also known as mathematical programming, is a broad field that covers various disciplines, including linear and nonlinear optimization, convex programming, integer programming, and more \cite{Kochenderfer2019}. Choosing an appropriate optimization method for a given problem is a crucial step, as problem characteristics such as nonlinearity, the presence of constraints, and the computational cost of evaluating the objective function can significantly influence the method's performance. In many cases, classic gradient-based algorithms—like Davidon-Fletcher-Powell \cite{Fletcher1963} or Broyden-Fletcher-Goldfarb-Shanno \cite{broyden1970}—can be efficiently used. However, they are less suitable, particularly when the gradient must be computed numerically—this can be time-consuming, computationally expensive, and not always highly accurate.

In this work, we solve a constrained, nonlinear optimization problem where the objective function is obtained numerically and may take several hours to evaluate. This setting naturally leads us toward gradient-free (or derivative-free) methods, which rely solely on objective function evaluations. Furthermore, in the presence of constraints, penalty and barrier methods provide systematic ways to handle feasible regions while maintaining the simplicity of an unconstrained search scheme.
\todo[inline]{Improve this bit.}

However, derivative-free methods must also be chosen carefully. Earlier research efforts \cite{buresBP, buresVU} explored coupling various optimization algorithms—including gradient-based methods—with the numerical simulations underlying our objective function. These studies showed that while gradient-based methods can be effective when gradients are well-defined and efficiently computed, they were less practical in our black-box context. In contrast, gradient-free approaches, which avoid the cost and complexity of gradient approximation, proved to be more robust and straightforward to implement. Among the tested approaches, the Nelder-Mead method outperformed alternative gradient-free algorithms, making it a natural choice for this work.

To further enhance our optimization framework, we also employ a direct search method known as Mesh Adaptive Direct Search (MADS). MADS has shown broad applicability in constrained black-box optimization problems and can be effectively integrated with penalty/barrier techniques. By comparing Nelder-Mead and MADS, we aim to leverage their complementary strengths—Nelder-Mead’s simplicity and MADS’s systematic handling of constraints and adaptive refinement—in a unified framework that addresses the computational challenges of our problem.

This chapter focuses on methods that address the specific challenges posed by the problem in this work. First, a general optimization problem is defined. Next, black-box optimization methods are discussed. Finally, the proposed optimization framework, including its components and their interconnections, is presented in detail.

\input{chapters/optimization/general_optimization_problem.tex}
\input{chapters/optimization/black_box.tex}
\input{chapters/optimization/optim_framework.tex}
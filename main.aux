\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\providecommand\babel@aux[2]{}
\@nameuse{bbl@beforestart}
\babel@aux{american}{}
\babel@aux{american}{}
\babel@aux{american}{}
\babel@aux{american}{}
\babel@aux{american}{}
\citation{Fletcher1963}
\citation{broyden1970}
\citation{Bert}
\citation{Bert}
\citation{Bert}
\citation{non-linear-textbook}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Mathematical optimization}{7}{chapter.1}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{optimization}{{1}{7}{Mathematical optimization}{chapter.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}General optimization problem}{7}{section.1.1}\protected@file@percent }
\newlabel{eq:admissible solution}{{1.1}{7}{General optimization problem}{equation.1.1.1}{}}
\newlabel{eq:basic problem}{{1.2}{7}{General optimization problem}{equation.1.1.2}{}}
\citation{Bert}
\citation{Bert}
\citation{non-linear-textbook}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Solving constrained problems}{8}{section.1.2}\protected@file@percent }
\newlabel{constrained}{{1.2}{8}{Solving constrained problems}{section.1.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.1}Penalty methods}{8}{subsection.1.2.1}\protected@file@percent }
\newlabel{penalty method}{{1.2.1}{8}{Penalty methods}{subsection.1.2.1}{}}
\newlabel{eq:penalty function}{{1.5}{8}{Penalty methods}{equation.1.2.5}{}}
\newlabel{eq:cost function with penalty}{{1.6}{8}{Penalty methods}{equation.1.2.6}{}}
\citation{non-linear-textbook}
\citation{non-linear-textbook}
\citation{non-linear-textbook}
\@writefile{lof}{\contentsline {figure}{\numberline {1.1}{\ignorespaces An illustration of the penalty method applied to minimizing the function $ f(x) = 0.5x $ with the constraint $ g(x) = 4 - x \leq 0 $. Different shapes of the modified objective function $ \phi (x, r) $ depending on the value of the penalty parameter $ r $ are distinguished by color. The condition defining the set of admissible solutions is indicated by the gray dashed line. The set of admissible solutions lies in the half-plane to the right of this gray dashed line.\relax }}{9}{figure.caption.1}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:penalty}{{1.1}{9}{An illustration of the penalty method applied to minimizing the function $ f(x) = 0.5x $ with the constraint $ g(x) = 4 - x \leq 0 $. Different shapes of the modified objective function $ \phi (x, r) $ depending on the value of the penalty parameter $ r $ are distinguished by color. The condition defining the set of admissible solutions is indicated by the gray dashed line. The set of admissible solutions lies in the half-plane to the right of this gray dashed line.\relax }{figure.caption.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.2}Barrier method}{9}{subsection.1.2.2}\protected@file@percent }
\newlabel{barrier method}{{1.2.2}{9}{Barrier method}{subsection.1.2.2}{}}
\newlabel{eq:log barrier function}{{1.10}{9}{Barrier method}{equation.1.2.10}{}}
\newlabel{eq:reciprocal barrier function}{{1.11}{9}{Barrier method}{equation.1.2.11}{}}
\newlabel{eq:cost function with barrier}{{1.12}{9}{Barrier method}{equation.1.2.12}{}}
\citation{BBO-textbook}
\@writefile{lof}{\contentsline {figure}{\numberline {1.2}{\ignorespaces An illustration of the barrier method applied to minimizing the function $ f(x) = 0.5x $ with the constraint $ g(x) = 4 - x \leq 0 $. Different shapes of the modified objective function $ \phi (x, r) $ depending on the value of the barrier parameter $ r $ are distinguished by color. The condition defining the set of feasible solutions is indicated by the gray dashed line. The set of feasible solutions lies in the half-plane to the right of this gray dashed line.\relax }}{10}{figure.caption.2}\protected@file@percent }
\newlabel{fig:barrier}{{1.2}{10}{An illustration of the barrier method applied to minimizing the function $ f(x) = 0.5x $ with the constraint $ g(x) = 4 - x \leq 0 $. Different shapes of the modified objective function $ \phi (x, r) $ depending on the value of the barrier parameter $ r $ are distinguished by color. The condition defining the set of feasible solutions is indicated by the gray dashed line. The set of feasible solutions lies in the half-plane to the right of this gray dashed line.\relax }{figure.caption.2}{}}
\newlabel{eq:extreme barrier}{{1.15}{10}{Barrier method}{equation.1.2.15}{}}
\citation{BBO-textbook}
\citation{BBO-textbook}
\citation{derivative-free-review}
\citation{two-decades}
\citation{BBO-textbook}
\citation{derivative-free-review}
\citation{Kramer2011}
\citation{BBO-textbook}
\citation{BBO-textbook}
\citation{BBO-textbook}
\citation{BBO-textbook}
\citation{BBO-textbook}
\citation{Dantzig1990}
\citation{Nelder1965}
\@writefile{toc}{\contentsline {section}{\numberline {1.3}Black-box optimization}{11}{section.1.3}\protected@file@percent }
\newlabel{black-box}{{1.3}{11}{Black-box optimization}{section.1.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.1}Heuristic Methods}{11}{subsection.1.3.1}\protected@file@percent }
\newlabel{heuristic}{{1.3.1}{11}{Heuristic Methods}{subsection.1.3.1}{}}
\citation{BBO-textbook}
\citation{BBO-textbook}
\citation{derivative-free-review}
\citation{Nelder1965}
\citation{BBO-textbook}
\@writefile{lof}{\contentsline {figure}{\numberline {1.3}{\ignorespaces A schematic representation of the operations used to transform simplexes in the Nelder-Mead method. The vertices generated by applying each operation are shown in red. For clarity, the operations are depicted in $ \mathbb  {R}^2 $.\relax }}{12}{figure.caption.3}\protected@file@percent }
\newlabel{fig:NM operations}{{1.3}{12}{A schematic representation of the operations used to transform simplexes in the Nelder-Mead method. The vertices generated by applying each operation are shown in red. For clarity, the operations are depicted in $ \mathbb {R}^2 $.\relax }{figure.caption.3}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces Nelder-Mead algorithm\relax }}{13}{algorithm.1}\protected@file@percent }
\newlabel{neldermead}{{1}{13}{Nelder-Mead algorithm\relax }{algorithm.1}{}}
\citation{Audet2002}
\citation{Audet2006}
\@writefile{lof}{\contentsline {figure}{\numberline {1.4}{\ignorespaces Several iterations of the Nelder-Mead method for a specific choice of the initial simplex when minimizing the function $ x^2 - 4x + y^2 - y - xy + 7 $, with the minimum at the point (3,2) marked by a red cross.\relax }}{14}{figure.caption.4}\protected@file@percent }
\newlabel{fig:NM}{{1.4}{14}{Several iterations of the Nelder-Mead method for a specific choice of the initial simplex when minimizing the function $ x^2 - 4x + y^2 - y - xy + 7 $, with the minimum at the point (3,2) marked by a red cross.\relax }{figure.caption.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.2}Direct search methods}{14}{subsection.1.3.2}\protected@file@percent }
\newlabel{direct-search}{{1.3.2}{14}{Direct search methods}{subsection.1.3.2}{}}
\citation{BBO-textbook}
\citation{Audet2002}
\citation{BBO-textbook}
\citation{Audet2002}
\citation{BBO-textbook}
\citation{Audet2002}
\citation{BBO-textbook}
\citation{BBO-textbook}
\@writefile{lof}{\contentsline {figure}{\numberline {1.5}{\ignorespaces Examples of search directions and meshes in $\mathbb  {R}^2$ with obtained by different choices of $\mathbf  {G}$ and~$\mathbf  {Z}$. The mesh points are at the intersections of the lines, the arrows represent possible search directions.\relax }}{15}{figure.caption.5}\protected@file@percent }
\newlabel{fig:gps}{{1.5}{15}{Examples of search directions and meshes in $\mathbb {R}^2$ with obtained by different choices of $\mathbf {G}$ and~$\mathbf {Z}$. The mesh points are at the intersections of the lines, the arrows represent possible search directions.\relax }{figure.caption.5}{}}
\citation{BBO-textbook}
\citation{derivative-free-review}
\citation{BBO-textbook}
\@writefile{loa}{\contentsline {algorithm}{\numberline {2}{\ignorespaces Generalized Pattern Search (GPS) for unconstrained optimization\relax }}{16}{algorithm.2}\protected@file@percent }
\newlabel{GPS-algo}{{2}{16}{Generalized Pattern Search (GPS) for unconstrained optimization\relax }{algorithm.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.6}{\ignorespaces Examples of meshes and frames $\mathbb  {R}^2$ for different values of $\delta ^k$ and $\Delta ^k$\relax }}{17}{figure.caption.6}\protected@file@percent }
\newlabel{fig:mads}{{1.6}{17}{Examples of meshes and frames $\mathbb {R}^2$ for different values of $\delta ^k$ and $\Delta ^k$\relax }{figure.caption.6}{}}
\citation{two-decades}
\citation{BBO-textbook}
\citation{Kramer2011}
\@writefile{loa}{\contentsline {algorithm}{\numberline {3}{\ignorespaces Mesh Adaptive Direct Search (MADS)\relax }}{18}{algorithm.3}\protected@file@percent }
\newlabel{MADS}{{3}{18}{Mesh Adaptive Direct Search (MADS)\relax }{algorithm.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.3}Optimization Using a Surrogate Model}{18}{subsection.1.3.3}\protected@file@percent }
\newlabel{model-based}{{1.3.3}{18}{Optimization Using a Surrogate Model}{subsection.1.3.3}{}}
\citation{BBO-textbook}
\citation{two-decades}
\citation{BBO-textbook}
\bibstyle{unsrt}
\bibdata{reference}
\@writefile{lof}{\contentsline {figure}{\numberline {1.7}{\ignorespaces An illustration of two surrogate models, $ s_1 $ and $ s_2 $. The black points represent noisy values of the objective function. While using surrogate model $ s_1 $ represents a better choice for approximating the function, it is not suitable for optimization since $ s_1 $ contains many undesirable stationary points that the original objective function does not have. On the other hand, while surrogate model $ s_2 $ is not as accurate in approximating the function's values, it is a better choice for optimization because the stationary points of $ s_2 $ are almost identical to those of the optimized objective function.\relax }}{19}{figure.caption.7}\protected@file@percent }
\newlabel{fig:surrogate}{{1.7}{19}{An illustration of two surrogate models, $ s_1 $ and $ s_2 $. The black points represent noisy values of the objective function. While using surrogate model $ s_1 $ represents a better choice for approximating the function, it is not suitable for optimization since $ s_1 $ contains many undesirable stationary points that the original objective function does not have. On the other hand, while surrogate model $ s_2 $ is not as accurate in approximating the function's values, it is a better choice for optimization because the stationary points of $ s_2 $ are almost identical to those of the optimized objective function.\relax }{figure.caption.7}{}}
\bibcite{Fletcher1963}{1}
\bibcite{broyden1970}{2}
\bibcite{Bert}{3}
\bibcite{non-linear-textbook}{4}
\bibcite{BBO-textbook}{5}
\bibcite{derivative-free-review}{6}
\bibcite{two-decades}{7}
\bibcite{Kramer2011}{8}
\bibcite{Dantzig1990}{9}
\bibcite{Nelder1965}{10}
\bibcite{Audet2002}{11}
\bibcite{Audet2006}{12}
\gdef \@abspage@last{20}
